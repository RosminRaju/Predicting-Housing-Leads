# Prediciting Housing Leads 
- In order to build a predictive model to predict the likelihood of a homeowner given a set of features, a significant amount of data processing was required before. The original dataset contained many missing values for a number of features. We dropped specific feature variables where 30 percent of its instance contained missing values ('NaN'). These features variables were: New Parent, Expectant Parent, and Newlywed. For the remaining feature variables, we implemented an imputation technique to fill in the missing values. When our imputer encountered missing values for a numerical feature, it would replace the missing value with the mean value corresponding to the missing value's respective numeric column. When it encountered missing values for a categorical feature, it would replace the missing value with the most frequent value corresponding to the missing value's respective categorical column. Since our machine learning tool, Scikit-learn, only accepts numbers as input, we had to represent our categorical features as numbers. In order to convert each categorical feature to a numerical representation, we used Scikit-learn's DictVectorizer, which generates a one-hot encoding of our textual categorical variables. Once we vectorized our categorical data, our data processing was complete, and we were able to generate a features matrix for our modeling process. As for our label generation, we created a positive label ("Move") for the case where the Current Home Purchased Date occurred after May 2015, and we created a negative label ("Not Move") for the case where the Current Home Purchased Date occurred before or on May 2015.

- Since we are originally working with mixed data (numerical and categorical data), modeling choices are limited, and we need to be cautious and creative with our selections. Hence, we first select a Random Forest Classifier as our initial model because of it's ability to perform well with mixed data. Our first Random Forest model has a precision, recall, f1-score, and accuracy score of 99%. However, as we can see in our confusion matrix plot (See iPython notebook), our first model is heavily imbalanced towards the true negative and false negative cases. As you might have guessed, the reason we get 99% accuracy on an imbalanced dataset (with 97% of the instances being negative) is because our model looks at the data and cleverly decide that the best thing to do is to always predict “Not Move” and achieve high accuracy. In other words, our accuracy score is only reflecting the underlying class distribution. This explains why we deceptively have a very high accuracy score. Furthermore, the high number of false negatives is very costly because we are missing potential moving leads. 

- To counteract these issues, we resample the dataset to offset the imbalance by under-sampling the majority class ('Not Move'). With our resampled dataset, we built a new Random Forest Classifier. As we can see in our confusion matrix plot for this model, there are higher diagonal values of the confusion matrix than non-diagonal values of the confusion matrix, which indicates that this random forest model has made many correct predictions. Additionally, there are less false negatives now after we balanced the classes. This is very good because it is costly for a real estate agent to miss a potential client. The precision, recall, and f1-score of the model are all 86% (with 66% of the instances being negative)! The cross_validated accuracy score of our Random Forest model was 83.5% +/- 2.1% for cv=5. As we can see from our performance metrics, resampling our dataset has given us a boost in our performance metrics. 

- We generated a feature importance plot (See iPython notebook) which reveals how to use a forest of decision trees to evaluate the importance of features on our moving classification task. The green horizontal bars are the feature importances of the forest. As we can see, there are 5 features that are most informative. They are: Marital Status in the Household, Home Loan to Value, Home Year Built, Home Square Footage, and Home Length of Residence.

- We experimented with boosting techniques to see if we could create a moving classification model that performed better than our Random Forest Classifier. Despite our best attempts, neither boosting classifier we built was able to outperform our Random Forest Classifier for our moving classification task.

- To sum up our predictive modeling process, we built a random forest classifier with high predictive performance after resampling our imbalanced dataset. Along with the predictive model we generate, we provide a list of the 100 most likely homeowners (those who did not move after May 2015) to move, which can be found in the provided csv file. 